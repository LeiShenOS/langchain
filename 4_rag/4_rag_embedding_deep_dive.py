# RAGåµŒå…¥æ¨¡å‹æ·±åº¦è§£æ
# æœ¬æ–‡ä»¶æ·±å…¥æ¢è®¨äº†RAGç³»ç»Ÿä¸­ä¸åŒåµŒå…¥æ¨¡å‹çš„ä½¿ç”¨å’Œæ¯”è¾ƒ
# åµŒå…¥æ¨¡å‹æ˜¯RAGç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ï¼Œè´Ÿè´£å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡è¡¨ç¤º
#
# åµŒå…¥æ¨¡å‹çš„é‡è¦æ€§ï¼š
# 1. è¯­ä¹‰ç†è§£ï¼šå°†æ–‡æœ¬è½¬æ¢ä¸ºèƒ½å¤Ÿæ•è·è¯­ä¹‰ä¿¡æ¯çš„å‘é‡
# 2. ç›¸ä¼¼åº¦è®¡ç®—ï¼šé€šè¿‡å‘é‡è·ç¦»è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦
# 3. æ£€ç´¢è´¨é‡ï¼šåµŒå…¥è´¨é‡ç›´æ¥å½±å“æ£€ç´¢çš„å‡†ç¡®æ€§
# 4. æˆæœ¬è€ƒè™‘ï¼šä¸åŒæ¨¡å‹åœ¨æˆæœ¬ã€æ€§èƒ½ã€éƒ¨ç½²æ–¹å¼ä¸Šæœ‰æ˜¾è‘—å·®å¼‚

import os

# å¯¼å…¥ä¸åŒçš„åµŒå…¥æ¨¡å‹
from langchain.embeddings import HuggingFaceEmbeddings  # Hugging Faceå¼€æºåµŒå…¥æ¨¡å‹
from langchain.text_splitter import CharacterTextSplitter  # æ–‡æœ¬åˆ†å‰²å™¨
from langchain_community.document_loaders import TextLoader  # æ–‡æœ¬åŠ è½½å™¨
from langchain_community.vectorstores import Chroma  # å‘é‡æ•°æ®åº“
from langchain_openai import OpenAIEmbeddings  # OpenAIåµŒå…¥æ¨¡å‹

# å®šä¹‰æ–‡ä»¶è·¯å¾„å’Œç›®å½•
current_dir = os.path.dirname(os.path.abspath(__file__))  # å½“å‰è„šæœ¬ç›®å½•
file_path = os.path.join(current_dir, "books", "odyssey.txt")  # å¥¥å¾·èµ›æ–‡æœ¬æ–‡ä»¶
db_dir = os.path.join(current_dir, "db")  # æ•°æ®åº“å­˜å‚¨ç›®å½•

# æ£€æŸ¥æ–‡æœ¬æ–‡ä»¶æ˜¯å¦å­˜åœ¨
if not os.path.exists(file_path):
    raise FileNotFoundError(
        f"æ–‡ä»¶ {file_path} ä¸å­˜åœ¨ã€‚è¯·æ£€æŸ¥è·¯å¾„ã€‚"
    )

# ä»æ–‡ä»¶ä¸­è¯»å–æ–‡æœ¬å†…å®¹
loader = TextLoader(file_path)
documents = loader.load()
print(f"åŠ è½½çš„æ–‡æ¡£æ•°é‡: {len(documents)}")

# å°†æ–‡æ¡£åˆ†å‰²æˆå—
# ä½¿ç”¨ç»Ÿä¸€çš„åˆ†å‰²ç­–ç•¥ï¼Œä»¥ä¾¿å…¬å¹³æ¯”è¾ƒä¸åŒåµŒå…¥æ¨¡å‹çš„æ•ˆæœ
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
docs = text_splitter.split_documents(documents)

# æ˜¾ç¤ºåˆ†å‰²åçš„æ–‡æ¡£ä¿¡æ¯
print("\n--- æ–‡æ¡£å—ä¿¡æ¯ ---")
print(f"æ–‡æ¡£å—æ•°é‡: {len(docs)}")
print(f"ç¤ºä¾‹æ–‡æ¡£å—:\n{docs[0].page_content[:200]}...\n")


# åˆ›å»ºå’ŒæŒä¹…åŒ–å‘é‡å­˜å‚¨çš„è¾…åŠ©å‡½æ•°
def create_vector_store(docs, embeddings, store_name):
    """
    åˆ›å»ºå‘é‡å­˜å‚¨çš„è¾…åŠ©å‡½æ•°

    å‚æ•°:
    docs: æ–‡æ¡£å—åˆ—è¡¨
    embeddings: åµŒå…¥æ¨¡å‹å®ä¾‹
    store_name: å­˜å‚¨åç§°ï¼Œç”¨äºåŒºåˆ†ä¸åŒåµŒå…¥æ¨¡å‹çš„æ•°æ®åº“
    """
    persistent_directory = os.path.join(db_dir, store_name)
    if not os.path.exists(persistent_directory):
        print(f"\n--- åˆ›å»ºå‘é‡å­˜å‚¨ {store_name} ---")
        print(f"ä½¿ç”¨åµŒå…¥æ¨¡å‹: {type(embeddings).__name__}")

        # åˆ›å»ºå‘é‡æ•°æ®åº“
        # è¿™ä¸ªè¿‡ç¨‹ä¼šä¸ºæ¯ä¸ªæ–‡æ¡£å—ç”ŸæˆåµŒå…¥å‘é‡
        db = Chroma.from_documents(
            docs, embeddings, persist_directory=persistent_directory)
        print(f"--- å®Œæˆåˆ›å»ºå‘é‡å­˜å‚¨ {store_name} ---")
        print(f"å‘é‡ç»´åº¦: {len(embeddings.embed_query('test'))} ç»´")
    else:
        print(f"å‘é‡å­˜å‚¨ {store_name} å·²å­˜åœ¨ã€‚æ— éœ€åˆå§‹åŒ–ã€‚")


# ========== ä¸åŒåµŒå…¥æ¨¡å‹çš„æ¯”è¾ƒ ==========

# 1. OpenAIåµŒå…¥æ¨¡å‹
# ä¼˜ç‚¹ï¼š
# - é«˜è´¨é‡çš„è¯­ä¹‰ç†è§£èƒ½åŠ›
# - ç»è¿‡å¤§è§„æ¨¡æ•°æ®è®­ç»ƒï¼Œæ³›åŒ–èƒ½åŠ›å¼º
# - APIè°ƒç”¨ç®€å•ï¼Œæ— éœ€æœ¬åœ°éƒ¨ç½²
# - æŒç»­æ›´æ–°å’Œä¼˜åŒ–
# ç¼ºç‚¹ï¼š
# - éœ€è¦ä»˜è´¹ä½¿ç”¨ï¼ˆæŒ‰tokenè®¡è´¹ï¼‰
# - ä¾èµ–ç½‘ç»œè¿æ¥
# - æ•°æ®éœ€è¦å‘é€åˆ°OpenAIæœåŠ¡å™¨
# é€‚ç”¨åœºæ™¯ï¼šå¯¹è´¨é‡è¦æ±‚é«˜ï¼Œé¢„ç®—å……è¶³çš„å•†ä¸šåº”ç”¨
print("\n--- ä½¿ç”¨OpenAIåµŒå…¥æ¨¡å‹ ---")
print("ç‰¹ç‚¹ï¼šå•†ä¸šçº§è´¨é‡ï¼ŒAPIè°ƒç”¨ï¼ŒæŒ‰ä½¿ç”¨ä»˜è´¹")
print("å®šä»·ä¿¡æ¯ï¼šhttps://openai.com/api/pricing/")

openai_embeddings = OpenAIEmbeddings(
    model="text-embedding-ada-002"  # OpenAIçš„ç»å…¸åµŒå…¥æ¨¡å‹
    # ä¹Ÿå¯ä»¥ä½¿ç”¨æ›´æ–°çš„æ¨¡å‹ï¼š
    # model="text-embedding-3-small"  # æ›´æ–°ã€æ›´ä¾¿å®œçš„å°å‹æ¨¡å‹
    # model="text-embedding-3-large"  # æœ€é«˜è´¨é‡çš„å¤§å‹æ¨¡å‹
)
create_vector_store(docs, openai_embeddings, "chroma_db_openai")

# 2. Hugging Faceå¼€æºåµŒå…¥æ¨¡å‹
# ä¼˜ç‚¹ï¼š
# - å®Œå…¨å…è´¹ä½¿ç”¨
# - æœ¬åœ°è¿è¡Œï¼Œæ•°æ®éšç§æ€§å¥½
# - æ¨¡å‹é€‰æ‹©ä¸°å¯Œï¼Œå¯é’ˆå¯¹ç‰¹å®šé¢†åŸŸä¼˜åŒ–
# - æ”¯æŒç¦»çº¿ä½¿ç”¨
# ç¼ºç‚¹ï¼š
# - éœ€è¦æœ¬åœ°è®¡ç®—èµ„æº
# - é¦–æ¬¡ä¸‹è½½æ¨¡å‹éœ€è¦æ—¶é—´
# - è´¨é‡å¯èƒ½ä¸å¦‚å•†ä¸šæ¨¡å‹
# é€‚ç”¨åœºæ™¯ï¼šé¢„ç®—æœ‰é™ã€å¯¹æ•°æ®éšç§è¦æ±‚é«˜ã€æˆ–éœ€è¦ç¦»çº¿è¿è¡Œçš„åº”ç”¨
print("\n--- ä½¿ç”¨Hugging Faceå¼€æºåµŒå…¥æ¨¡å‹ ---")
print("ç‰¹ç‚¹ï¼šå…è´¹å¼€æºï¼Œæœ¬åœ°è¿è¡Œï¼Œä¿æŠ¤æ•°æ®éšç§")
print("æ›´å¤šæ¨¡å‹ï¼šhttps://huggingface.co/models?other=embeddings")

huggingface_embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-mpnet-base-v2"
    # è¿™æ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„é€šç”¨åµŒå…¥æ¨¡å‹
    # å…¶ä»–æ¨èæ¨¡å‹ï¼š
    # "sentence-transformers/all-MiniLM-L6-v2"  # æ›´å°æ›´å¿«çš„æ¨¡å‹
    # "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"  # å¤šè¯­è¨€æ”¯æŒ
    # "sentence-transformers/distilbert-base-nli-stsb-mean-tokens"  # åŸºäºDistilBERT
)
create_vector_store(docs, huggingface_embeddings, "chroma_db_huggingface")

print("\nâœ… OpenAIå’ŒHugging FaceåµŒå…¥æ¨¡å‹æ¼”ç¤ºå®Œæˆ")

# åµŒå…¥æ¨¡å‹é€‰æ‹©å»ºè®®ï¼š
# 1. å¦‚æœé¢„ç®—å……è¶³ä¸”å¯¹è´¨é‡è¦æ±‚é«˜ â†’ OpenAIåµŒå…¥æ¨¡å‹
# 2. å¦‚æœéœ€è¦æ§åˆ¶æˆæœ¬æˆ–ä¿æŠ¤æ•°æ®éšç§ â†’ Hugging Faceæ¨¡å‹
# 3. å¦‚æœéœ€è¦ç‰¹å®šé¢†åŸŸä¼˜åŒ– â†’ å¯»æ‰¾ä¸“é—¨çš„Hugging Faceæ¨¡å‹
# 4. å¦‚æœéœ€è¦å¤šè¯­è¨€æ”¯æŒ â†’ é€‰æ‹©å¤šè¯­è¨€åµŒå…¥æ¨¡å‹


# ========== åµŒå…¥æ¨¡å‹æ•ˆæœæ¯”è¾ƒ ==========

# æŸ¥è¯¢å‘é‡å­˜å‚¨çš„è¾…åŠ©å‡½æ•°
def query_vector_store(store_name, query, embedding_function):
    """
    æŸ¥è¯¢ç‰¹å®šå‘é‡å­˜å‚¨å¹¶æ˜¾ç¤ºç»“æœ

    å‚æ•°:
    store_name: å‘é‡å­˜å‚¨åç§°
    query: æŸ¥è¯¢é—®é¢˜
    embedding_function: åµŒå…¥å‡½æ•°ï¼ˆå¿…é¡»ä¸åˆ›å»ºæ—¶ä½¿ç”¨çš„ç›¸åŒï¼‰
    """
    persistent_directory = os.path.join(db_dir, store_name)
    if os.path.exists(persistent_directory):
        print(f"\n{'='*60}")
        print(f"æŸ¥è¯¢å‘é‡å­˜å‚¨: {store_name}")
        print(f"åµŒå…¥æ¨¡å‹: {type(embedding_function).__name__}")
        print(f"{'='*60}")

        # åŠ è½½å‘é‡æ•°æ®åº“
        # æ³¨æ„ï¼šå¿…é¡»ä½¿ç”¨ä¸åˆ›å»ºæ—¶ç›¸åŒçš„åµŒå…¥å‡½æ•°
        db = Chroma(
            persist_directory=persistent_directory,
            embedding_function=embedding_function,
        )

        # é…ç½®æ£€ç´¢å™¨
        retriever = db.as_retriever(
            search_type="similarity_score_threshold",
            search_kwargs={
                "k": 3,  # è¿”å›å‰3ä¸ªæœ€ç›¸å…³çš„ç»“æœ
                "score_threshold": 0.1  # ç›¸ä¼¼åº¦é˜ˆå€¼
            },
        )

        # æ‰§è¡Œæ£€ç´¢
        relevant_docs = retriever.invoke(query)

        # æ˜¾ç¤ºæ£€ç´¢ç»“æœ
        if relevant_docs:
            print(f"æ‰¾åˆ° {len(relevant_docs)} ä¸ªç›¸å…³æ–‡æ¡£")
            for i, doc in enumerate(relevant_docs, 1):
                print(f"\næ–‡æ¡£ {i}:")
                print(f"å†…å®¹é•¿åº¦: {len(doc.page_content)} å­—ç¬¦")
                print(f"å†…å®¹: {doc.page_content[:300]}...")
                if doc.metadata:
                    print(f"æ¥æº: {doc.metadata.get('source', 'æœªçŸ¥')}")
        else:
            print("æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")

    else:
        print(f"å‘é‡å­˜å‚¨ {store_name} ä¸å­˜åœ¨ã€‚")


# å®šä¹‰æµ‹è¯•æŸ¥è¯¢
# ä½¿ç”¨å…³äºå¥¥å¾·èµ›çš„é—®é¢˜æ¥æµ‹è¯•ä¸åŒåµŒå…¥æ¨¡å‹çš„æ£€ç´¢æ•ˆæœ
query = "Who is Odysseus' wife?"  # å¥¥å¾·ä¿®æ–¯çš„å¦»å­æ˜¯è°ï¼Ÿ

print(f"\n{'#'*80}")
print(f"æµ‹è¯•æŸ¥è¯¢: {query}")
print(f"æ¯”è¾ƒä¸åŒåµŒå…¥æ¨¡å‹çš„æ£€ç´¢æ•ˆæœ")
print(f"{'#'*80}")

# ä½¿ç”¨ä¸åŒåµŒå…¥æ¨¡å‹æŸ¥è¯¢ç›¸åŒé—®é¢˜ï¼Œæ¯”è¾ƒç»“æœè´¨é‡
print("\nğŸ” åµŒå…¥æ¨¡å‹æ•ˆæœæ¯”è¾ƒ:")

# 1. OpenAIåµŒå…¥æ¨¡å‹ç»“æœ
query_vector_store("chroma_db_openai", query, openai_embeddings)

# 2. Hugging FaceåµŒå…¥æ¨¡å‹ç»“æœ
query_vector_store("chroma_db_huggingface", query, huggingface_embeddings)

print(f"\n{'#'*80}")
print("åµŒå…¥æ¨¡å‹æ¯”è¾ƒæ€»ç»“:")
print("1. OpenAIæ¨¡å‹ï¼šé€šå¸¸æä¾›æ›´é«˜è´¨é‡çš„è¯­ä¹‰ç†è§£")
print("2. Hugging Faceæ¨¡å‹ï¼šå…è´¹ä¸”å¯æœ¬åœ°è¿è¡Œï¼Œè´¨é‡ä¹Ÿå¾ˆä¸é”™")
print("3. é€‰æ‹©å»ºè®®ï¼šæ ¹æ®é¢„ç®—ã€éšç§éœ€æ±‚å’Œè´¨é‡è¦æ±‚æ¥é€‰æ‹©")
print("4. æ€§èƒ½æµ‹è¯•ï¼šå»ºè®®åœ¨å®é™…æ•°æ®ä¸Šæµ‹è¯•ä¸åŒæ¨¡å‹çš„æ•ˆæœ")
print(f"{'#'*80}")

print("\nâœ… æŸ¥è¯¢æ¼”ç¤ºå®Œæˆ")

# å®é™…åº”ç”¨ä¸­çš„è€ƒè™‘å› ç´ ï¼š
# 1. æˆæœ¬ï¼šOpenAIæŒ‰ä½¿ç”¨é‡æ”¶è´¹ï¼ŒHugging Faceå…è´¹ä½†éœ€è¦è®¡ç®—èµ„æº
# 2. å»¶è¿Ÿï¼šæœ¬åœ°æ¨¡å‹é¦–æ¬¡åŠ è½½è¾ƒæ…¢ï¼Œä½†åç»­æŸ¥è¯¢æ›´å¿«
# 3. è´¨é‡ï¼šé€šå¸¸OpenAIæ¨¡å‹è´¨é‡æ›´é«˜ï¼Œä½†å·®è·åœ¨ç¼©å°
# 4. éšç§ï¼šæœ¬åœ°æ¨¡å‹ä¸ä¼šå°†æ•°æ®å‘é€åˆ°å¤–éƒ¨æœåŠ¡å™¨
# 5. å¯å®šåˆ¶æ€§ï¼šå¼€æºæ¨¡å‹å¯ä»¥é’ˆå¯¹ç‰¹å®šé¢†åŸŸè¿›è¡Œå¾®è°ƒ
